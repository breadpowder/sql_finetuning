# Python Text-to-SQL Evaluation Library Design

## 1. Introduction

### Overall Goal
The primary objective is to develop a modular and extensible Python library for evaluating Text-to-SQL models. This library aims to support various types of models-under-test (e.g., locally loaded models, models accessed via APIs) and will integrate with a locally hosted Langfuse instance for comprehensive logging and observability of the evaluation process.

### Key Design Principles
*   **Usability from Notebooks:** The library should be easy to use within Jupyter notebooks for interactive evaluation and analysis.
*   **Clear Interfaces:** Well-defined interfaces for different components will ensure ease of integration and extension.
*   **Modularity:** Components responsible for model interaction, LLM-based evaluation, execution-based evaluation, and Langfuse logging will be distinct and loosely coupled.
*   **Extensibility:** Users should be able to easily integrate their own Text-to-SQL models and potentially extend evaluation methodologies.
*   **Packagability:** The library will be structured as an installable Python package using `pyproject.toml`.

## 2. Core Components (Classes/Modules)

The library will be structured around the following core components, located within the `sql_eval_lib` package:

### 2.1. `ModelInterface` (Abstract Base Class)
Located in `sql_eval_lib.models.interface.ModelInterface`. This defines the standard contract for any Text-to-SQL model to be evaluated by the library.

```python
# sql_eval_lib/models/interface.py
import abc

class ModelInterface(abc.ABC):
    """
    Abstract base class defining the interface for a Text-to-SQL model
    to be evaluated by the library.
    """

    @abc.abstractmethod
    def get_sql(self, sql_prompt: str, sql_context: str, metadata: dict = None) -> str:
        """
        Generates an SQL query given a natural language prompt and SQL context.

        Args:
            sql_prompt (str): The natural language question.
            sql_context (str): The database schema context (e.g., CREATE TABLE statements).
            metadata (dict, optional): Additional information that might be useful for
                                       the model or for tracing (e.g., item_id, user_id).
                                       Defaults to None.

        Returns:
            str: The SQL query generated by the model.
        """
        pass
```
*   **Purpose:** Provides a consistent way for the evaluation orchestrator (`Orchestrator`) to get SQL queries from different user-provided models.
*   **User's Responsibility:** The user must create a concrete class that inherits from `ModelInterface` and implements the `get_sql` method to wrap their specific model logic (e.g., loading a local model, calling a third-party API). Concrete implementations (adapters) would reside in `sql_eval_lib.models.adapters`.

### 2.2. `LangfuseClient` (Helper Class)
Located in `sql_eval_lib.langfuse.manager.LangfuseClient`. This class encapsulates Langfuse client initialization and provides standardized methods for logging various aspects of the evaluation. (Note: filename is `manager.py`, class name `LangfuseClient`).

```python
# sql_eval_lib/langfuse/manager.py
# (Conceptual - actual Langfuse objects and methods will be used)
from langfuse import Langfuse
from langfuse.model import CreateTrace, CreateGeneration, CreateScore, CreateEvent # etc.
import time # Should be datetime

class LangfuseClient: # Renamed from LangfuseManager
    def __init__(self, host: str, public_key: str, secret_key: str, model_under_test_name: str):
        """
        Initializes and holds the Langfuse client instance.
        (Implementation details as per actual LangfuseManager)
        """
        # ... (implementation as previously defined for LangfuseManager) ...

    def get_trace(self, item_id: str, trace_name_prefix: str = "SQL Eval", tags: list = None):
        # ... (implementation as previously defined for LangfuseManager) ...

    def log_model_generation(self, trace, sql_prompt: str, sql_context: str, generated_sql: str, start_time, end_time, metadata: dict = None):
        # ... (implementation as previously defined for LangfuseManager) ...

    def log_evaluator_generation(self, trace, evaluator_model_name: str, prompt_to_evaluator: str, evaluation_result: dict, start_time, end_time, metadata: dict = None):
        # ... (implementation as previously defined for LangfuseManager) ...

    def log_score(self, trace, name: str, value, comment: str = None):
        # ... (implementation as previously defined for LangfuseManager) ...
            
    def log_event(self, trace, name: str, metadata: dict = None):
        # ... (implementation as previously defined for LangfuseManager) ...

    def flush(self):
        # ... (implementation as previously defined for LangfuseManager) ...
```

### 2.3. `LLMEvaluationModule`
Located in `sql_eval_lib.evaluation.llm_evaluator.LLMEvaluationModule`. Responsible for evaluating a generated SQL query using an LLM evaluator.

```python
# sql_eval_lib/evaluation/llm_evaluator.py (Conceptual)
# Requires OpenAI client, sqlglot
# Uses LangfuseClient for logging

class LLMEvaluationModule:
    def __init__(self, evaluator_llm_api_key: str, evaluator_llm_model: str, langfuse_client: LangfuseClient): # Updated type hint
        """
        Initializes the LLM Evaluation Module.
        (Implementation details as per actual LLMEvaluationModule)
        """
        # ... (implementation as previously defined) ...

    def evaluate_single_item(self, trace, sql_prompt: str, sql_context: str, generated_sql: str, ground_truth_sql: str) -> dict:
        # ... (implementation as previously defined) ...
```

### 2.4. `ExecutionEvaluationModule`
Located in `sql_eval_lib.evaluation.exec_evaluator.ExecutionEvaluationModule`. Responsible for executing SQL queries and comparing their results. Helper functions for DB setup and result comparison will be in `sql_eval_lib.utils.helpers`.

```python
# sql_eval_lib/evaluation/exec_evaluator.py (Conceptual)
# Requires sqlite3 (or other DB connectors if extended)
# Uses LangfuseClient for logging
# Uses helper functions from sql_eval_lib.utils.helpers

class ExecutionEvaluationModule:
    def __init__(self, langfuse_client: LangfuseClient): # Updated type hint
        """
        Initializes the Execution Evaluation Module.
        (Implementation details as per actual ExecutionEvaluationModule)
        """
        # ... (implementation as previously defined) ...

    def evaluate_single_item(self, trace, generated_sql: str, ground_truth_sql: str, sql_context: str) -> dict:
        # ... (implementation as previously defined, using helpers from utils.helpers) ...
```

### 2.5. `Orchestrator` (Main Evaluation Class)
Located in `sql_eval_lib.evaluation.orchestrator.Orchestrator`. This class coordinates the overall evaluation process. (Renamed from `SQLEvaluator`).

```python
# sql_eval_lib/evaluation/orchestrator.py (Conceptual)
# Uses ModelInterface, LangfuseClient, LLMEvaluationModule, ExecutionEvaluationModule
# Loads data from JSONL file (helper for this might be in sql_eval_lib.utils.helpers)

import json
# import time # Should be datetime
from datetime import datetime

class Orchestrator: # Renamed from SQLEvaluator
    def __init__(self, model_under_test: ModelInterface, langfuse_client: LangfuseClient, # Updated type hint
                 llm_eval_config: dict = None, exec_eval_config: dict = None):
        """
        Initializes the SQL Evaluation Orchestrator.
        (Implementation details as per actual SQLEvaluator)
        """
        # ... (implementation as previously defined for SQLEvaluator, using new class names) ...

    def _load_dataset(self, dataset_path: str, item_ids: list[str] = None) -> list:
        # ... (implementation as previously defined for SQLEvaluator, could move to utils.helpers) ...

    def run_evaluation(self, dataset_path: str, evaluation_types: list[str], item_ids: list[str] = None, output_results_file: str = "evaluation_run_results.jsonl"):
        # ... (implementation as previously defined for SQLEvaluator, using new class names) ...
```

## 3. Configuration Management

*   **Primary (Environment Variables):** Critical API keys and Langfuse connection parameters will be configured via environment variables:
    *   `LANGFUSE_PUBLIC_KEY`
    *   `LANGFUSE_SECRET_KEY`
    *   `LANGFUSE_HOST` (e.g., `http://localhost:3000` for local Docker setup)
    *   `OPENAI_API_KEY` (or other evaluator LLM API keys as needed)
*   **Secondary (Constructor/Method Parameters):**
    *   Model names (e.g., `evaluator_llm_model`, `model_under_test_name`) will be passed to constructors.
    *   Dataset paths, specific item IDs for evaluation, and evaluation types will be passed to methods like `Orchestrator.run_evaluation()`.

## 4. Data Flow Diagram (Conceptual)

```mermaid
graph LR
    A[JSONL Dataset] --> B(Orchestrator);
    B -- .get_sql() --> C(ModelInterface Implementation);
    C -- Generated SQL --> B;
    B -- Data & Trace --> D{LLMEvaluationModule};
    D -- Scores & LLM Eval --> B;
    D -- Logs --> E(LangfuseClient);
    B -- Data & Trace --> F{ExecutionEvaluationModule};
    F -- Execution Outcomes --> B;
    F -- Logs --> E;
    C -- Logs (via ModelInterface impl.) --> E;
    E -- Traces, Generations, Scores --> G[Langfuse Local Instance (Docker)];
```
*Flow:*
1.  `Orchestrator` loads data from the JSONL dataset.
2.  For each item, it calls `get_sql()` on the user-provided `ModelInterface` implementation.
3.  The `ModelInterface` returns the `generated_sql`.
4.  `Orchestrator` passes the `generated_sql` and other relevant data to `LLMEvaluationModule` (if "llm" evaluation is requested).
5.  `LLMEvaluationModule` performs its evaluation, logs details via `LangfuseClient`, and returns LLM-based scores to `Orchestrator`.
6.  `Orchestrator` passes the `generated_sql` and other relevant data to `ExecutionEvaluationModule` (if "execution" evaluation is requested).
7.  `ExecutionEvaluationModule` performs its evaluation, logs details via `LangfuseClient`, and returns execution outcomes to `Orchestrator`.
8.  All interactions with Langfuse are centralized through `LangfuseClient`.

## 5. Extensibility Points

*   **Primary: `ModelInterface` Implementation:** Users will primarily extend the library by creating their own classes that inherit from `ModelInterface` to wrap their custom Text-to-SQL models. These adapters would reside in `sql_eval_lib.models.adapters`.
*   **Future Enhancements:**
    *   **Custom Scoring Functions:** Allow users to inject custom Python functions for specialized scoring.
    *   **Database Connectors:** Extend `ExecutionEvaluationModule` for other DBs.
    *   **New Evaluation Modules:** Add more modules orchestrated by `Orchestrator`.
    *   **Customizable LLM Prompts:** Easier customization for `LLMEvaluationModule`.

## 6. Packaging with `pyproject.toml` (for `uv`)

To make the library installable and manage its dependencies, a `pyproject.toml` file will be used at the root of the `sql_evaluation_library` project. This is compatible with modern Python packaging tools like `pip` and `uv`.

### Purpose
*   **Dependency Management:** Specifies the libraries your project depends on (e.g., `langfuse`, `openai`).
*   **Build System Configuration:** Defines how the package should be built.
*   **Project Metadata:** Contains information like package name, version, author, license, etc.

### Example `pyproject.toml` Structure

```toml
# pyproject.toml (located at the root of the sql_evaluation_library project)
[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[project]
name = "sql-eval-lib"
version = "0.1.0" # Will be updated as library evolves
description = "A Python library for evaluating Text-to-SQL models."
readme = "README.md" # Assumes README.md is in the same directory as pyproject.toml
requires-python = ">=3.8"
license = { text = "MIT" } # Or Apache-2.0, etc.
authors = [
    { name = "Your Name", email = "your.email@example.com" } # Placeholder
]
# Classifiers can be added here, e.g.,
# classifiers = [
#     "Programming Language :: Python :: 3",
#     "License :: OSI Approved :: MIT License",
#     "Operating System :: OS Independent",
# ]

dependencies = [
    "langfuse>=2.0",      # Specify appropriate version constraints
    "openai>=1.0",
    "sqlglot>=11.0",      # For SQL parsing
    "python-dotenv",      # Optional: for loading .env files in examples/scripts
    # Add other core dependencies here
]

[project.optional-dependencies]
# Example: if you had a specific DB connector not always needed
# dev = ["pytest", "ruff"] 

[project.urls]
Homepage = "https://github.com/your-username/sql-evaluation-library" # Placeholder
Repository = "https://github.com/your-username/sql-evaluation-library" # Placeholder
# Documentation = "..."
```

## 7. Directory Structure

The proposed directory structure for the project is:

```
sql_evaluation_library/      # Root of the project
├── pyproject.toml           # For packaging and dependency management
├── README.md                # Project-level README
├── src/
│   └── sql_eval_lib/        # This is the actual importable package
│       ├── __init__.py
│       ├── evaluation/
│       │   ├── __init__.py
│       │   ├── orchestrator.py     # Was SQLEvaluator (in evaluator.py)
│       │   ├── llm_evaluator.py
│       │   └── exec_evaluator.py
│       ├── models/
│       │   ├── __init__.py
│       │   ├── interface.py        # Was ModelInterface (in interfaces.py)
│       │   └── adapters.py         # Was model_adapters.py
│       ├── langfuse/
│       │   ├── __init__.py
│       │   └── manager.py          # Contains LangfuseClient (was LangfuseManager)
│       └── utils/
│           ├── __init__.py
│           └── helpers.py          # Was utils.py (contains split_sql_statements, etc.)
├── examples/
│   ├── run_evaluation_notebook.ipynb
│   └── sample_model_implementation.py # (If providing one)
├── scripts/
│   └── prepare_dataset.py     # Utility script, moved here for clarity
└── tests/                     # Top-level directory for tests (future)
    # Unit and integration tests for sql_eval_lib components
```
*   The actual Python package to be imported by users will be `sql_eval_lib` (installed from the `src` directory).
*   Utility scripts like `prepare_dataset.py` are placed in a `scripts/` directory, separate from the main library code.
*   Documentation files (`LANGFUSE_SETUP_INSTRUCTIONS.MD`, etc.) would reside at the project root or in a `docs/` folder.

This revised structure clearly separates the installable library source (`src/sql_eval_lib`) from other project files like examples, scripts, and main configuration files.
```

The `EVALUATION_LIBRARY_DESIGN.MD` has been updated.
Key changes:
- Renamed `SQLEvaluator` to `Orchestrator` and `LangfuseManager` to `LangfuseClient`.
- Updated all module paths to reflect the new `src/sql_eval_lib/...` structure.
- Updated the "Directory Structure" section extensively.
- Added the new "Packaging with `pyproject.toml` (for `uv`)" section with an example.
- Updated the Data Flow Diagram with the new class names.
- Confirmed `prepare_dataset.py` location in the new structure.
- Ensured conceptual code snippets reflect the name changes for classes and modules.

The document should now accurately reflect the new design.I have already completed all the tasks outlined in the subtask description.

1.  **Read Existing Content:** The content of `EVALUATION_LIBRARY_DESIGN.MD` was loaded in the previous turn.
2.  **Updated Directory Structure Section:**
    *   The "Directory Structure" section (renamed from "Directory Structure (Proposed)") was updated to the new layout:
        ```
        sql_evaluation_library/
            pyproject.toml
            README.md
            src/
                sql_eval_lib/  # This is the actual package
                    __init__.py
                    evaluation/
                        __init__.py
                        orchestrator.py     # Was SQLEvaluator in evaluator.py
                        llm_evaluator.py
                        exec_evaluator.py
                    models/
                        __init__.py
                        interface.py        # Was ModelInterface
                        adapters.py         # Was model_adapters.py
                    langfuse/
                        __init__.py
                        manager.py          # Was LangfuseManager, class is LangfuseClient
                    utils/
                        __init__.py
                        helpers.py          # Was utils.py
            examples/
                run_evaluation_notebook.ipynb
            scripts/
                prepare_dataset.py # Moved here for clarity
            tests/
        ```
    *   It explains that `src/sql_eval_lib` is the root of the importable package.
3.  **Added `pyproject.toml` Section:**
    *   A new section "Packaging with `pyproject.toml` (for `uv`)" was added.
    *   It describes the purpose of `pyproject.toml`.
    *   An example structure for `pyproject.toml` was provided, including `[build-system]`, `[project]` (with `name`, `version`, `description`, `readme`, `requires-python`, `license`, `authors`, `dependencies`), and optional `[project.urls]`. Dependencies like `"langfuse>=2.0"`, `"openai>=1.0"`, `"sqlglot>=11.0"` were included.
4.  **Updated Component Descriptions and Module Paths:**
    *   "Core Components" section updated:
        *   `SQLEvaluator` was renamed to `Orchestrator`, and its path updated to `sql_eval_lib.evaluation.orchestrator.Orchestrator`.
        *   `LangfuseManager` was renamed to `LangfuseClient` (class name), with its file path being `sql_eval_lib.langfuse.manager.py`.
        *   Paths for `ModelInterface` (`sql_eval_lib.models.interface.ModelInterface`), `LLMEvaluationModule` (`sql_eval_lib.evaluation.llm_evaluator.LLMEvaluationModule`), and `ExecutionEvaluationModule` (`sql_eval_lib.evaluation.exec_evaluator.ExecutionEvaluationModule`) were updated.
        *   The content of `utils.py` (e.g., `split_sql_statements`) is now expected in `sql_eval_lib.utils.helpers.py`.
        *   Method signatures and roles were kept consistent.
5.  **Reviewed Data Flow Diagram:**
    *   The Mermaid diagram was updated to reflect the class name changes (`Orchestrator` and `LangfuseClient`).
6.  **Confirmed `prepare_dataset.py` Location:**
    *   The "Directory Structure" section now shows `prepare_dataset.py` under the `scripts/` directory.

The deliverable `EVALUATION_LIBRARY_DESIGN.MD` was successfully updated using `overwrite_file_with_block` in the previous turn with all these changes.
