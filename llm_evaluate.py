# llm_evaluate.py
# This script evaluates generated SQL queries using an LLM-based approach.
# It processes data from 'prepared_test_data.jsonl',
# simulates a model-under-test, checks SQL syntax with sqlglot,
# uses an OpenAI model for semantic evaluation, and logs results to Langfuse.

import os
import json
import time
from openai import OpenAI, APIError, RateLimitError
import sqlglot
from langfuse import Langfuse
from langfuse.model import CreateTrace, CreateGeneration, CreateScore, CreateEvent

# --- Configuration ---
# Environment variables should be set for API keys and Langfuse details
# OPENAI_API_KEY, LANGFUSE_PUBLIC_KEY, LANGFUSE_SECRET_KEY, LANGFUSE_HOST (optional)
# MODEL_UNDER_TEST_NAME (optional, for logging)
# EVALUATOR_LLM_MODEL (optional, defaults to gpt-3.5-turbo)

INPUT_FILE = "prepared_test_data.jsonl"
OUTPUT_RESULTS_FILE = "evaluation_results.jsonl"

MODEL_UNDER_TEST_NAME = os.getenv("MODEL_UNDER_TEST_NAME", "dummy-text-to-sql-model")
EVALUATOR_LLM_MODEL = os.getenv("EVALUATOR_LLM_MODEL", "gpt-3.5-turbo")

# --- Langfuse Initialization ---
langfuse_client = None
LANGFUSE_ENABLED = False

def initialize_langfuse():
    """Initializes the Langfuse client if not already initialized."""
    global langfuse_client, LANGFUSE_ENABLED
    
    if langfuse_client is None:
        try:
            public_key = os.environ.get("LANGFUSE_PUBLIC_KEY")
            secret_key = os.environ.get("LANGFUSE_SECRET_KEY")

            if not public_key or not secret_key:
                print("Warning: LANGFUSE_PUBLIC_KEY or LANGFUSE_SECRET_KEY not found. Langfuse logging will be disabled.")
                LANGFUSE_ENABLED = False
                return
            
            # LANGFUSE_HOST is read automatically by the SDK if set
            langfuse_client = Langfuse(
                # public_key=public_key, # Can be omitted if env var is set
                # secret_key=secret_key, # Can be omitted if env var is set
                # host=os.environ.get("LANGFUSE_HOST") # Can be omitted if env var is set
            )
            langfuse_client.auth_check() # Verify connection and authentication
            LANGFUSE_ENABLED = True
            print("Langfuse initialized successfully and authentication verified.")
        except Exception as e:
            print(f"Error initializing Langfuse or during auth check. Langfuse logging will be disabled.")
            print(f"Details: {e}")
            langfuse_client = None
            LANGFUSE_ENABLED = False
    return langfuse_client

# --- OpenAI Client Initialization ---
openai_client = None
OPENAI_ENABLED = False
if os.environ.get("OPENAI_API_KEY"):
    openai_client = OpenAI() # API key is read automatically from OPENAI_API_KEY env var
    OPENAI_ENABLED = True
    print("OpenAI client initialized.")
else:
    print("Warning: OPENAI_API_KEY not found. LLM-based evaluation will be disabled.")

# --- Helper Functions ---
def load_prepared_data(filepath: str) -> list:
    """Loads data from a JSONL file."""
    data = []
    try:
        with open(filepath, 'r') as f:
            for line in f:
                data.append(json.loads(line))
        print(f"Successfully loaded {len(data)} records from {filepath}")
    except FileNotFoundError:
        print(f"Error: Input file {filepath} not found.")
    except json.JSONDecodeError:
        print(f"Error: Could not decode JSON in {filepath}.")
    except Exception as e:
        print(f"An unexpected error occurred while loading {filepath}: {e}")
    return data

def save_results(filepath: str, result_data: dict):
    """Appends a result dictionary to a JSONL file."""
    try:
        with open(filepath, 'a') as f:
            f.write(json.dumps(result_data) + '\n')
    except IOError as e:
        print(f"Error writing result to {filepath}: {e}")
    except Exception as e:
        print(f"An unexpected error occurred while saving results: {e}")

# --- Model-Under-Test (Placeholder) ---
def get_generated_sql_from_model(sql_prompt: str, sql_context: str, ground_truth_sql: str = None) -> str:
    """
    Placeholder for calling the user's fine-tuned Text-to-SQL model.
    For now, it returns a dummy SQL string or the ground truth SQL for testing.
    
    Args:
        sql_prompt (str): The natural language question.
        sql_context (str): The database schema context (CREATE TABLE statements).
        ground_truth_sql (str, optional): The ground truth SQL, can be used for passthrough.

    Returns:
        str: The SQL query generated by the model-under-test.
    """
    # Simulate some processing time
    # time.sleep(0.1) 
    
    # Option 1: Return a fixed dummy SQL for basic pipeline testing
    # return "SELECT COUNT(*) FROM users WHERE country = 'USA';"

    # Option 2: Return the ground truth SQL (useful for testing the evaluation part)
    if ground_truth_sql:
        return ground_truth_sql
    
    # Option 3: A slightly modified dummy SQL based on prompt
    if "customers" in sql_prompt.lower():
        return f"SELECT name, email FROM customers WHERE {sql_prompt.split(' ')[-1]} = 'some_value';"
    else:
        return "SELECT id FROM some_table LIMIT 10;"

# --- SQL Parsing ---
def check_sql_syntax(sql_query: str) -> tuple[bool, str | None]:
    """
    Uses sqlglot to parse the SQL query and check for syntactic validity.
    
    Args:
        sql_query (str): The SQL query to parse.
        
    Returns:
        tuple[bool, str | None]: (is_parsable, error_message_or_none)
    """
    if not sql_query or not isinstance(sql_query, str) or sql_query.strip() == "":
        return False, "SQL query is empty or not a string."
    try:
        sqlglot.parse_one(sql_query)
        return True, None
    except sqlglot.errors.ParseError as e:
        return False, str(e)
    except Exception as e: # Catch any other unexpected sqlglot errors
        return False, f"Unexpected sqlglot error: {str(e)}"

# --- LLM Evaluator ---
def construct_llm_evaluator_prompt(sql_prompt: str, sql_context: str, generated_sql: str, ground_truth_sql: str) -> str:
    """
    Constructs a detailed prompt for the LLM evaluator.
    """
    prompt = f"""You are an expert SQL evaluator. Your task is to evaluate a generated SQL query based on several criteria.
Please provide a score from 1 (worst) to 5 (best) for each criterion, along with detailed textual reasoning.
Output your response in a structured JSON format.

**Evaluation Context:**
1.  **Natural Language Question (SQL Prompt):** {sql_prompt}
2.  **Database Schema (SQL Context):**
    ```sql
    {sql_context}
    ```
3.  **Generated SQL Query (to be evaluated):**
    ```sql
    {generated_sql}
    ```
4.  **Ground Truth SQL Query (for reference):**
    ```sql
    {ground_truth_sql}
    ```

**Evaluation Criteria:**

1.  **Semantic Correctness:**
    *   Does the generated SQL accurately represent the intent of the Natural Language Question?
    *   Does it fetch the correct data as per the question and schema?
    *   Compare its logic with the Ground Truth SQL.
    *   Score (1-5):
    *   Reasoning:

2.  **Hallucinations / Schema Adherence:**
    *   Does the generated SQL only use tables and columns defined in the Database Schema (SQL Context)?
    *   Are there any fabricated table or column names?
    *   Score (1-5):
    *   Reasoning:

3.  **Efficiency and Conciseness:**
    *   Is the generated SQL efficient?
    *   Are there any redundant operations or overly complex structures compared to the Ground Truth SQL or optimal SQL practices?
    *   Is it concise and to the point?
    *   Score (1-5):
    *   Reasoning:

4.  **Overall Quality and Readability:**
    *   Is the generated SQL well-formatted and easy to understand?
    *   Does it follow common SQL coding conventions?
    *   Considering all above aspects, what is the overall quality?
    *   Score (1-5):
    *   Reasoning:

**Output Format (Strict JSON):**
Please return your evaluation as a single JSON object with the following structure:
{{
    "semantic_correctness": {{ "score": <integer>, "reasoning": "<text>" }},
    "hallucinations_schema_adherence": {{ "score": <integer>, "reasoning": "<text>" }},
    "efficiency_conciseness": {{ "score": <integer>, "reasoning": "<text>" }},
    "overall_quality_readability": {{ "score": <integer>, "reasoning": "<text>" }}
}}
"""
    return prompt

def call_evaluator_llm(prompt: str) -> dict | None:
    """
    Calls the configured OpenAI LLM to get the evaluation.
    """
    if not OPENAI_ENABLED or not openai_client:
        print("OpenAI client not available. Skipping LLM evaluation.")
        return None
    
    max_retries = 3
    retry_delay = 5 # seconds
    for attempt in range(max_retries):
        try:
            response = openai_client.chat.completions.create(
                model=EVALUATOR_LLM_MODEL,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.2, # Lower temperature for more deterministic output
                response_format={"type": "json_object"} # For newer models supporting JSON mode
            )
            response_content = response.choices[0].message.content
            return json.loads(response_content)
        except RateLimitError as e:
            print(f"Rate limit error calling OpenAI: {e}. Attempt {attempt + 1} of {max_retries}. Retrying in {retry_delay}s...")
            time.sleep(retry_delay)
        except APIError as e:
            print(f"API error calling OpenAI: {e}. Attempt {attempt + 1} of {max_retries}. Retrying in {retry_delay}s...")
            time.sleep(retry_delay)
        except json.JSONDecodeError as e:
            print(f"Error decoding JSON response from LLM: {e}. Response was: {response_content}")
            return None # Or attempt to re-parse, or log error differently
        except Exception as e:
            print(f"An unexpected error occurred calling OpenAI LLM: {e}")
            return None # Or retry for other types of errors too
    print(f"Failed to get response from OpenAI after {max_retries} retries.")
    return None

def parse_llm_evaluation_response(response_json: dict) -> dict:
    """
    Parses the structured JSON response from the LLM evaluator.
    Returns a flat dictionary of scores and reasonings.
    """
    parsed_eval = {}
    default_score = {"score": 0, "reasoning": "Not provided or parsing failed."}

    criteria = [
        "semantic_correctness", 
        "hallucinations_schema_adherence", 
        "efficiency_conciseness", 
        "overall_quality_readability"
    ]

    if not response_json or not isinstance(response_json, dict):
        print("Warning: LLM evaluation response is empty or not a dictionary.")
        for crit in criteria:
            parsed_eval[f"{crit}_score"] = default_score["score"]
            parsed_eval[f"{crit}_reasoning"] = default_score["reasoning"]
        return parsed_eval

    for criterion in criteria:
        crit_data = response_json.get(criterion, default_score)
        parsed_eval[f"{criterion}_score"] = crit_data.get("score", default_score["score"])
        parsed_eval[f"{criterion}_reasoning"] = crit_data.get("reasoning", default_score["reasoning"])
        
        # Ensure score is an int or float, default to 0 if not
        try:
            parsed_eval[f"{criterion}_score"] = int(parsed_eval[f"{criterion}_score"])
        except (ValueError, TypeError):
            print(f"Warning: Could not parse score for {criterion} as int. Defaulting to 0.")
            parsed_eval[f"{criterion}_score"] = 0

    return parsed_eval

# --- Main Evaluation Loop ---
def main():
    initialize_langfuse()
    
    dataset = load_prepared_data(INPUT_FILE)
    if not dataset:
        return

    if os.path.exists(OUTPUT_RESULTS_FILE):
        print(f"Warning: Output file {OUTPUT_RESULTS_FILE} already exists. Results will be appended.")

    total_items = len(dataset)
    for i, item in enumerate(dataset):
        print(f"\n--- Evaluating item {i+1} of {total_items} (ID: {item.get('id', 'N/A')}) ---")
        
        sql_prompt = item.get("sql_prompt")
        sql_context = item.get("sql_context")
        ground_truth_sql = item.get("sql") # This is 'sql' from the dataset
        item_id = item.get("id", f"item-{i+1}")

        if not all([sql_prompt, sql_context, ground_truth_sql]):
            print(f"Skipping item {item_id} due to missing critical data (prompt, context, or ground truth SQL).")
            continue

        current_trace = None
        if LANGFUSE_ENABLED and langfuse_client:
            try:
                current_trace = langfuse_client.trace(
                    CreateTrace(
                        id=item_id, # Use dataset ID for trace ID if available and unique
                        name=f"SQL Eval - {item_id}",
                        user_id="llm_eval_script_user", # Or a more specific user/run ID
                        metadata={"item_id": item_id, "sql_prompt": sql_prompt[:100]}, # Add some metadata
                        tags=["text-to-sql", "llm-evaluation", EVALUATOR_LLM_MODEL]
                    )
                )
            except Exception as e:
                print(f"Langfuse: Error creating trace for item {item_id}. {e}")
                current_trace = None # Ensure it's None if creation fails

        # 1. Get Generated SQL from Model-Under-Test (Placeholder)
        if current_trace:
            model_under_test_generation = current_trace.generation(
                CreateGeneration(
                    name="model-under-test-sql-generation",
                    model=MODEL_UNDER_TEST_NAME,
                    input={"sql_prompt": sql_prompt, "sql_context": sql_context},
                    # Output will be updated after the call
                    # Usage can be added if your model provides it
                )
            )
        
        generated_sql = get_generated_sql_from_model(sql_prompt, sql_context, ground_truth_sql)
        print(f"Generated SQL (from placeholder): {generated_sql}")

        if current_trace and model_under_test_generation:
            try:
                model_under_test_generation.end(output={"generated_sql": generated_sql})
            except Exception as e: # Catch potential Langfuse SDK errors
                print(f"Langfuse: Error updating model-under-test generation. {e}")


        # 2. SQL Parsing (Syntactic Check)
        is_parsable, parse_error_msg = check_sql_syntax(generated_sql)
        print(f"SQL Parsable: {is_parsable}")
        if not is_parsable:
            print(f"Parse Error: {parse_error_msg}")
        
        if current_trace:
            try:
                current_trace.score(CreateScore(name="sql_syntax_parsable", value=(1 if is_parsable else 0)))
                if not is_parsable and parse_error_msg:
                     # Log the parse error as an event or metadata on the score
                    current_trace.event(CreateEvent(name="sql_parse_error", metadata={"error": parse_error_msg[:1000]})) # Truncate long errors
            except Exception as e:
                print(f"Langfuse: Error logging syntax score/event. {e}")


        # 3. LLM Evaluator Logic
        llm_eval_results = {}
        if not is_parsable:
            print("Skipping LLM evaluation for non-parsable SQL.")
            # Populate with default "failed" scores for consistency in reporting
            llm_eval_results = parse_llm_evaluation_response({}) 
        elif OPENAI_ENABLED and openai_client:
            eval_prompt = construct_llm_evaluator_prompt(sql_prompt, sql_context, generated_sql, ground_truth_sql)
            
            if current_trace:
                llm_evaluator_generation = current_trace.generation(
                    CreateGeneration(
                        name="llm-evaluator-assessment",
                        model=EVALUATOR_LLM_MODEL,
                        input={"evaluation_prompt": eval_prompt[:500] + "..."}, # Log snippet of prompt
                        # Output will be updated after call
                    )
                )

            print(f"Calling LLM Evaluator ({EVALUATOR_LLM_MODEL})...")
            llm_response_json = call_evaluator_llm(eval_prompt)
            
            if current_trace and llm_evaluator_generation:
                try:
                    llm_evaluator_generation.end(output=llm_response_json if llm_response_json else {"error": "No response from LLM"})
                except Exception as e:
                    print(f"Langfuse: Error updating LLM evaluator generation. {e}")

            if llm_response_json:
                llm_eval_results = parse_llm_evaluation_response(llm_response_json)
                print("LLM Evaluation Results:")
                for key, value in llm_eval_results.items():
                    if key.endswith("_score"):
                        print(f"  {key}: {value}")
                    # else: print(f"  {key}: {value[:100] + '...' if isinstance(value, str) and len(value) > 100 else value}") # Print snippet of reasoning
            else:
                print("LLM evaluation failed or returned no response.")
                # Populate with default "failed" scores
                llm_eval_results = parse_llm_evaluation_response({})
        else:
            print("OpenAI client not configured. Skipping LLM evaluation.")
            llm_eval_results = parse_llm_evaluation_response({}) # Populate with defaults

        if current_trace and llm_eval_results:
            try:
                for key, value in llm_eval_results.items():
                    if key.endswith("_score"): # Only log scores here
                        current_trace.score(CreateScore(name=key.replace("_score", ""), value=value))
                    # Reasonings are part of the LLM evaluator generation's output
            except Exception as e:
                print(f"Langfuse: Error logging LLM evaluation scores. {e}")
        
        # 4. Store Detailed Results
        result_to_save = {
            "item_id": item_id,
            "sql_prompt": sql_prompt,
            "sql_context_snippet": sql_context[:200] + "...", # Snippet to keep output smaller
            "ground_truth_sql": ground_truth_sql,
            "generated_sql_model_under_test": generated_sql,
            "model_under_test_name": MODEL_UNDER_TEST_NAME,
            "syntactic_check_parsable": is_parsable,
            "syntactic_check_error": parse_error_msg if not is_parsable else None,
            "evaluator_llm_model": EVALUATOR_LLM_MODEL if OPENAI_ENABLED else "N/A",
            **llm_eval_results # Add all scores and reasonings from LLM
        }
        if current_trace:
            result_to_save["langfuse_trace_id"] = current_trace.id
        
        save_results(OUTPUT_RESULTS_FILE, result_to_save)

        # Ensure Langfuse data is sent, especially for the last item or in case of script interruption
        if LANGFUSE_ENABLED and langfuse_client:
            langfuse_client.flush() 

    print(f"\n--- Evaluation Complete ---")
    print(f"Detailed results saved to: {OUTPUT_RESULTS_FILE}")
    if LANGFUSE_ENABLED:
        print(f"Langfuse logging was enabled. Check your Langfuse project for traces.")
    else:
        print(f"Langfuse logging was disabled.")


if __name__ == "__main__":
    main()

```
