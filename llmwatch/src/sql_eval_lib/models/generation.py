"""
SQL Generation using LangGraph and configured LLMs.

This module provides the core logic for generating SQL queries based on
natural language prompts and database schema context. It utilizes LangGraph
to define a structured generation process and integrates with configured
LLM models (e.g., OpenAI, Ollama) through the central configuration system.
"""

from typing import Dict, Any, TypedDict, Optional

from langchain_core.prompts import ChatPromptTemplate
from langchain_core.pydantic_v1 import BaseModel, Field as PydanticField # type: ignore
from langchain_openai import ChatOpenAI
from langgraph.graph import StateGraph, END

from sql_eval_lib.config import SQLEvalConfig, get_config

# --- System Prompts ---
SQL_GENERATION_SYSTEM_PROMPT = (
    "You are an expert SQL writer. Given a database schema (CREATE TABLE statements) "
    "and a natural language question, your task is to write a syntactically correct "
    "SQL query that accurately answers the question based on the provided schema. "
    "Ensure the query is efficient and directly addresses the question. "
    "Only output the SQL query, with no additional explanation or markdown."
)

# --- LangGraph State ---
class GraphState(TypedDict):
    """
    Represents the state of the SQL generation graph.

    Attributes:
        sql_prompt: The natural language question.
        sql_context: The database schema context.
        model_config: Configuration for the LLM.
        generated_sql: The SQL query generated by the LLM.
        error: Any error message encountered during generation.
    """
    sql_prompt: str
    sql_context: str
    model_config: Dict[str, Any]
    generated_sql: Optional[str]
    error: Optional[str]

# --- LangGraph Nodes ---
def llm_sql_generation_node(state: GraphState) -> GraphState:
    """
    Node that calls the LLM to generate an SQL query.

    Args:
        state: The current graph state.

    Returns:
        The updated graph state with the generated SQL or an error.
    """
    try:
        config: SQLEvalConfig = get_config() # Get the full SQLEvalConfig
        model_settings = config.model

        # For now, we assume OpenAI, but this could be extended
        # based on model_settings.adapter_type
        if model_settings.adapter_type.lower() != "openai":
            # This is a simplified error handling for now.
            # Ideally, we'd have different nodes or a factory for different model types.
            raise NotImplementedError(
                f"Model adapter '{model_settings.adapter_type}' is not yet supported in this node. "
                "Only 'openai' is currently implemented."
            )
        
        if not model_settings.api_key:
            raise ValueError("OpenAI API key is not configured.")

        llm = ChatOpenAI(
            api_key=model_settings.api_key.get_secret_value(),
            model=model_settings.model_name,
            temperature=model_settings.temperature,
            max_tokens=model_settings.max_tokens,
            # model_kwargs={"timeout": model_settings.timeout} # Pass timeout if API supports it this way
        )

        prompt = ChatPromptTemplate.from_messages([
            ("system", SQL_GENERATION_SYSTEM_PROMPT),
            ("human", "Database Schema:\n{sql_context}\n\nQuestion:\n{sql_prompt}")
        ])

        chain = prompt | llm
        
        response = chain.invoke({
            "sql_context": state["sql_context"],
            "sql_prompt": state["sql_prompt"]
        })
        
        generated_sql = response.content.strip()
        # Remove potential markdown code blocks if LLM wraps output
        if generated_sql.startswith("```sql"):
            generated_sql = generated_sql[len("```sql"):].strip()
        if generated_sql.startswith("```"): # Generic code block
             generated_sql = generated_sql[len("```"):].strip()
        if generated_sql.endswith("```"):
            generated_sql = generated_sql[:-len("```")].strip()
            
        return {**state, "generated_sql": generated_sql, "error": None}

    except Exception as e:
        error_message = f"Error during SQL generation: {str(e)}"
        # Consider logging the error here as well
        print(f"[llm_sql_generation_node ERROR]: {error_message}") # Added print for visibility
        return {**state, "generated_sql": None, "error": error_message}

# --- Graph Definition ---
def build_sql_generation_graph() -> StateGraph:
    """
    Builds and returns the LangGraph for SQL generation.
    """
    workflow = StateGraph(GraphState)
    workflow.add_node("generate_sql", llm_sql_generation_node)
    workflow.set_entry_point("generate_sql")
    workflow.add_edge("generate_sql", END)
    return workflow

# Compile the graph once when the module is loaded
COMPILED_SQL_GENERATION_GRAPH = build_sql_generation_graph().compile()


def get_generated_sql_from_model(sql_prompt: str, sql_context: str) -> str:
    """
    Generates an SQL query using a LangGraph-based workflow.

    This function takes a natural language SQL prompt and schema context,
    invokes a pre-compiled LangGraph, and returns the generated SQL query.
    It relies on the global application configuration for LLM settings.

    Args:
        sql_prompt: The natural language question or command.
        sql_context: The database schema context (e.g., CREATE TABLE statements).

    Returns:
        The generated SQL query as a string. If an error occurs during
        generation, a placeholder SQL query "SELECT 'Error in SQL generation; check logs.';" 
        is returned.
    
    Raises:
        NotImplementedError: If the configured model adapter is not supported.
        ValueError: If necessary configurations like API key are missing.
        # Other exceptions from Langchain/LangGraph or the LLM API might propagate.
    """
    app_config = get_config()
    model_config_dict = app_config.model.dict()

    initial_state: GraphState = {
        "sql_prompt": sql_prompt,
        "sql_context": sql_context,
        "model_config": model_config_dict, # model_config is part of state, but not directly used by node now
        "generated_sql": None,
        "error": None
    }

    # Ensure get_config() has been called if .env needs to be loaded by Pydantic BaseSettings
    # This is a bit of a workaround if the config isn't loaded globally before this call.
    # Typically, config loading should happen at application startup.
    if not app_config.model.api_key and app_config.model.adapter_type.lower() == "openai":
        print("Warning: OpenAI API key might not be loaded. Attempting to reload config.")
        app_config = get_config(config_file=app_config.config_file) # Reload with potential file
        if not app_config.model.api_key:
            print("Error: OpenAI API key is still not available after config reload.")
            return "SELECT 'Error: OpenAI API Key not configured.';" 

    try:
        final_state = COMPILED_SQL_GENERATION_GRAPH.invoke(initial_state)

        if final_state.get("error"):
            # Error is already printed in the node, could add more logging here
            print(f"[get_generated_sql_from_model ERROR]: {final_state['error']}")
            return "SELECT 'Error in SQL generation; check logs.';" 
        
        generated_sql = final_state.get("generated_sql")
        if generated_sql:
            return generated_sql
        else:
            print("[get_generated_sql_from_model WARNING]: SQL generation completed without error but no SQL was produced.")
            return "SELECT 'No SQL generated; check logs.';"
            
    except Exception as e:
        # Catch any unexpected errors during graph invocation itself
        print(f"[get_generated_sql_from_model CRITICAL ERROR]: Unexpected error invoking graph: {e}")
        return "SELECT 'Critical error invoking SQL generation graph; check logs.';"

# The __main__ block has been removed as per plan.
# Test functionality will be moved to a dedicated test file
# under the /tests directory.
