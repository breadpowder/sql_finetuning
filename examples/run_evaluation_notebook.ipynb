{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text-to-SQL Model Evaluation using `sql-eval-lib`\n",
    "\n",
    "This notebook demonstrates how to use the `sql-eval-lib` (SQL Evaluation Library) to evaluate a Text-to-SQL model. It covers setting up the environment, installing the library, initializing the necessary components, running evaluations (both LLM-based and execution-based), and inspecting the results.\n",
    "\n",
    "**Prerequisites:**\n",
    "1.  **`prepared_test_data.jsonl`**: This file should exist in the main project directory (one level above this `examples` folder). It is generated by running the `scripts/prepare_dataset.py` script from the main project directory.\n",
    "2.  **Langfuse Running Locally**: A Langfuse instance should be running locally via Docker. Refer to `LANGFUSE_SETUP_INSTRUCTIONS.MD` in the main project directory for setup details. Ensure it's accessible, typically at `http://localhost:3000`.\n",
    "3.  **Python Environment with `uv`**: This notebook assumes you are using `uv` for package management. Ensure `uv` is installed. The `sql-eval-lib` and its dependencies (`openai`, `sqlglot`, `langfuse`) will be installed in the next step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Install the `sql-eval-lib` Package\n",
    "\n",
    "Before running the rest of the notebook, ensure you have installed the `sql-eval-lib` package. If you are running this notebook from the `examples` directory within the `sql_evaluation_library` project, you can install it in editable mode using `uv` from the **project root directory** (i.e., one level up from this `examples` folder).\n",
    "\n",
    "**Open your terminal, navigate to the project root (`sql_evaluation_library/`), and run:**\n",
    "```bash\n",
    "# Ensure your virtual environment (e.g., .venv created by 'uv venv') is activated\n",
    "# source .venv/bin/activate \n",
    "uv pip install -e .\n",
    "```\n",
    "After installation, you might need to **restart the Jupyter kernel** for the changes to take effect and the library to be found."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Environment Variables\n",
    "\n",
    "Ensure the following environment variables are set in your terminal session or a `.env` file (if you load it, e.g., using `python-dotenv` before starting Jupyter). For sensitive keys, using environment variables is recommended.\n",
    "\n",
    "```bash\n",
    "# For Langfuse (ensure these match your Langfuse Docker setup)\n",
    "export LANGFUSE_PUBLIC_KEY=\"your_chosen_public_key_for_docker_pk_prefix\" # e.g., pk-lf-...\n",
    "export LANGFUSE_SECRET_KEY=\"your_chosen_strong_secret_key_for_docker\"\n",
    "export LANGFUSE_HOST=\"http://localhost:3000\"\n",
    "\n",
    "# For OpenAIModelAdapter and LLM-based evaluation (if used)\n",
    "export OPENAI_API_KEY=\"your_openai_api_key_here\" # e.g., sk-...\n",
    "\n",
    "# For identifying your model runs\n",
    "export MODEL_UNDER_TEST_NAME=\"MyDummySQLModel_NotebookRun\"\n",
    "\n",
    "# For the LLM evaluator model (if using LLM-based evaluation)\n",
    "export EVALUATOR_LLM_MODEL=\"gpt-3.5-turbo\" # Or \"gpt-4-turbo-preview\", etc.\n",
    "```\n",
    "\n",
    "**Note:** Replace placeholder values with your actual keys and desired names. If you don't set these as environment variables, you'll need to pass them as parameters directly when initializing the library components in the code cells below (though API keys are best kept out of code)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "# import sys # No longer needed as we assume package installation\n",
    "\n",
    "# Ensure your Python environment can find the installed 'sql_eval_lib'\n",
    "try:\n",
    "    from sql_eval_lib import (\n",
    "        LangfuseClient,     # Updated from LangfuseManager\n",
    "        Orchestrator,       # Updated from SQLEvaluator\n",
    "        DummyModelAdapter,\n",
    "        OpenAIModelAdapter,\n",
    "        ModelInterface      # Optional: for type hinting or custom model definition\n",
    "    )\n",
    "    print(\"Successfully imported components from 'sql_eval_lib'.\")\n",
    "except ImportError as e:\n",
    "    print(f\"Error importing from sql_eval_lib: {e}\")\n",
    "    print(\"Please ensure the library is correctly installed by running 'uv pip install -e .' from the project root.\")\n",
    "    print(\"You might need to restart the Jupyter kernel after installation.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "Load environment variables and define paths.\n",
    "**Note on `dataset_path`**: This notebook assumes `prepared_test_data.jsonl` is located in the project's root directory (i.e., one level up from this `examples` folder). This is the default output location for `scripts/prepare_dataset.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from environment variables (provide defaults or handle if not set)\n",
    "langfuse_public_key = os.getenv(\"LANGFUSE_PUBLIC_KEY\")\n",
    "langfuse_secret_key = os.getenv(\"LANGFUSE_SECRET_KEY\")\n",
    "langfuse_host = os.getenv(\"LANGFUSE_HOST\", \"http://localhost:3000\")\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "model_under_test_name = os.getenv(\"MODEL_UNDER_TEST_NAME\", \"MyNotebookSQLModel\")\n",
    "evaluator_llm_model_name = os.getenv(\"EVALUATOR_LLM_MODEL\", \"gpt-3.5-turbo\")\n",
    "\n",
    "# Define dataset path (assuming notebook is in 'examples/' and data is in project root)\n",
    "project_root = Path.cwd().parent\n",
    "dataset_path = project_root / \"prepared_test_data.jsonl\"\n",
    "output_results_path = Path.cwd() / \"notebook_evaluation_results.jsonl\" # Save results in examples folder\n",
    "\n",
    "print(f\"LANGFUSE_HOST: {langfuse_host}\")\n",
    "print(f\"MODEL_UNDER_TEST_NAME: {model_under_test_name}\")\n",
    "print(f\"EVALUATOR_LLM_MODEL: {evaluator_llm_model_name}\")\n",
    "print(f\"DATASET_PATH: {dataset_path}\")\n",
    "print(f\"OUTPUT_RESULTS_PATH: {output_results_path}\")\n",
    "print(f\"OpenAI API Key Loaded: {'Yes' if openai_api_key else 'No'}\")\n",
    "print(f\"Langfuse Public Key Loaded: {'Yes' if langfuse_public_key else 'No'}\")\n",
    "print(f\"Langfuse Secret Key Loaded: {'Yes' if langfuse_secret_key else 'No'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initialize LangfuseClient\n",
    "\n",
    "This client handles all communication with your Langfuse instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "langfuse_client_instance = None # Renamed variable for clarity\n",
    "if langfuse_public_key and langfuse_secret_key and langfuse_host:\n",
    "    try:\n",
    "        langfuse_client_instance = LangfuseClient( # Updated class name\n",
    "            model_under_test_name=model_under_test_name,\n",
    "            langfuse_public_key=langfuse_public_key,\n",
    "            langfuse_secret_key=langfuse_secret_key,\n",
    "            langfuse_host=langfuse_host\n",
    "        )\n",
    "        print(\"LangfuseClient initialized successfully.\")\n",
    "    except ConnectionError as e: # LangfuseClient raises ConnectionError on auth fail\n",
    "        print(f\"Failed to initialize LangfuseClient: {e}\")\n",
    "        print(\"Langfuse logging will be disabled. Ensure Langfuse is running and keys/host are correct.\")\n",
    "    except ValueError as e: # LangfuseClient raises ValueError if keys are missing\n",
    "        print(f\"Failed to initialize LangfuseClient due to missing configuration: {e}\")\n",
    "        print(\"Langfuse logging will be disabled.\")\n",
    "else:\n",
    "    print(\"Langfuse keys/host not fully configured. LangfuseClient will not be initialized, and logging will be disabled.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Initialize Model Adapter\n",
    "\n",
    "Choose and initialize the model adapter for the Text-to-SQL model you want to evaluate. \n",
    "We'll set up a `DummyModelAdapter` and an `OpenAIModelAdapter` (if configured)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_model = DummyModelAdapter(fixed_query=\"SELECT * FROM dummy_table_notebook;\")\n",
    "print(\"DummyModelAdapter instance created.\")\n",
    "\n",
    "open_ai_model_instance = None\n",
    "if openai_api_key:\n",
    "    try:\n",
    "        open_ai_model_instance = OpenAIModelAdapter(api_key=openai_api_key, model_name=\"gpt-3.5-turbo\") # Or use evaluator_llm_model_name\n",
    "        print(f\"OpenAIModelAdapter instance created for model: {open_ai_model_instance.model_name}\")\n",
    "    except ValueError as e: # OpenAIModelAdapter raises ValueError if API key is missing after check\n",
    "        print(f\"Could not initialize OpenAIModelAdapter: {e}\")\n",
    "else:\n",
    "    print(\"OpenAI API key not available, skipping OpenAIModelAdapter initialization.\")\n",
    "\n",
    "# Choose which model to use for the evaluation run\n",
    "# For this example, we'll prioritize OpenAI model if available, otherwise use the dummy model.\n",
    "model_to_evaluate = open_ai_model_instance or dummy_model\n",
    "print(f\"\\nUsing model adapter: {model_to_evaluate.__class__.__name__} for evaluation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Initialize Orchestrator\n",
    "\n",
    "This orchestrator class uses the model adapter and Langfuse client to run evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_eval_module_config = None\n",
    "if openai_api_key: # Only configure LLM eval if OpenAI key is available\n",
    "    llm_eval_module_config = {\n",
    "        \"api_key\": openai_api_key, \n",
    "        \"model_name\": evaluator_llm_model_name\n",
    "    }\n",
    "    print(f\"LLM Evaluation Module will be configured with model: {evaluator_llm_model_name}\")\n",
    "else:\n",
    "    print(\"OpenAI API key not available. LLM-based evaluation will be skipped by Orchestrator if requested.\")\n",
    "\n",
    "# exec_eval_config is an empty dict, meaning ExecutionEvaluationModule will be enabled by default if requested.\n",
    "exec_module_config = {}\n",
    "\n",
    "eval_orchestrator = Orchestrator( # Updated class name\n",
    "    model_under_test=model_to_evaluate, \n",
    "    langfuse_client=langfuse_client_instance, # Pass the initialized (or None) client, updated param name\n",
    "    llm_eval_config=llm_eval_module_config, \n",
    "    exec_eval_config=exec_module_config\n",
    ")\n",
    "\n",
    "print(\"Orchestrator initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Run Evaluation\n",
    "\n",
    "Specify which evaluation types to run. You can also select a subset of item IDs for a quick test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if dataset file exists before running\n",
    "if not dataset_path.exists():\n",
    "    print(f\"ERROR: Dataset file not found at {dataset_path}.\")\n",
    "    print(f\"Please run `scripts/prepare_dataset.py` from the project root directory ('{project_root}') first.\")\n",
    "    results = [] # Ensure results is defined\n",
    "else:\n",
    "    evaluation_types_to_run = [\"llm\", \"execution\"] # Can be [\"llm\"], [\"execution\"], or both\n",
    "    if not openai_api_key and \"llm\" in evaluation_types_to_run:\n",
    "        print(\"Warning: 'llm' evaluation requested but OpenAI API key is missing. LLM evaluation will be skipped.\")\n",
    "\n",
    "    print(f\"Starting evaluation for types: {evaluation_types_to_run}\")\n",
    "    \n",
    "    # Example: to run on a small subset, ensure these IDs exist in your prepared_test_data.jsonl\n",
    "    # test_item_ids = [\"1\", \"2\", \"3\"] \n",
    "    test_item_ids = None # Set to None to run on all items\n",
    "\n",
    "    from datetime import datetime\n",
    "    current_session_id = f\"notebook-run-{datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    print(f\"Using session ID for Langfuse: {current_session_id}\")\n",
    "\n",
    "    results = eval_orchestrator.run_evaluation(\n",
    "        dataset_path=str(dataset_path),\n",
    "        evaluation_types=evaluation_types_to_run,\n",
    "        item_ids=test_item_ids,\n",
    "        session_id=current_session_id,\n",
    "        output_file=str(output_results_path)\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nEvaluation complete. {len(results)} items processed.\")\n",
    "    if output_results_path.exists():\n",
    "        print(f\"Detailed results saved to: {output_results_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Inspect Results\n",
    "\n",
    "Let's look at the structure of the results for the first evaluated item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if results:\n",
    "    first_item_result = results[0]\n",
    "    print(\"Results for the first evaluated item:\")\n",
    "    print(json.dumps(first_item_result, indent=2))\n",
    "    \n",
    "    print(\"\\nKey fields in the result dictionary typically include:\")\n",
    "    print(\"- 'item_id': Identifier from the dataset.\")\n",
    "    print(\"- 'sql_prompt': The input natural language query.\")\n",
    "    print(\"- 'ground_truth_sql': The reference SQL query.\")\n",
    "    print(\"- 'generated_sql': The SQL query produced by your model-under-test.\")\n",
    "    if 'llm_evaluation' in first_item_result:\n",
    "        print(\"- 'llm_evaluation': Contains scores and reasoning from the LLM evaluator, such as:\")\n",
    "        print(\"  - 'llm_eval_parsable_score': Whether sqlglot found the generated SQL parsable before LLM eval.\")\n",
    "        print(\"  - 'semantic_correctness_score', 'semantic_correctness_reasoning', etc.\")\n",
    "    if 'execution_evaluation' in first_item_result:\n",
    "        print(\"- 'execution_evaluation': Contains outcomes from executing the SQL queries, such as:\")\n",
    "        print(\"  - 'generated_sql_execution_success', 'generated_sql_error', 'generated_sql_output'.\")\n",
    "        print(\"  - 'ground_truth_sql_execution_success', 'ground_truth_sql_error', 'ground_truth_sql_output'.\")\n",
    "        print(\"  - 'results_match': Boolean indicating if the outputs of generated and ground truth SQL matched.\")\n",
    "    # Langfuse trace ID is part of the model_metadata_for_call within Orchestrator\n",
    "    # and not directly added to the item_result by default in the current Orchestrator design.\n",
    "    # Users can check Langfuse UI using session_id or item_id for traces.\n",
    "    if langfuse_client_instance and langfuse_client_instance.enabled and first_item_result.get('item_id'):\n",
    "        print(f\"\\nTo find this item in Langfuse, search for traces with item_id='{first_item_result.get('item_id')}' or session_id='{current_session_id}'.\")\n",
    "else:\n",
    "    print(\"No results to display. Ensure the evaluation ran correctly and the dataset was not empty.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Next Steps\n",
    "\n",
    "*   **Explore Langfuse**: Open your local Langfuse instance (usually `http://localhost:3000`) in your browser. You should see traces for each evaluated item, along with detailed generations, scores, and any logged metadata. This provides a rich UI for debugging and analysis.\n",
    "*   **Analyze Results Files**: \n",
    "    *   The `notebook_evaluation_results.jsonl` file in this `examples` directory contains the structured output for each item processed in this notebook run.\n",
    "    *   Refer to `ANALYZE_LLM_EVALUATION_RESULTS.md` and `ANALYZE_EXECUTION_EVALUATION_RESULTS.md` in the main project directory for more detailed guidance on interpreting these results and using Langfuse for deeper analysis.\n",
    "*   **Customize**: \n",
    "    *   Modify this notebook to use your own custom model adapter by creating a class that implements `ModelInterface`.\n",
    "    *   Experiment with different `evaluation_types` or by selecting specific `test_item_ids`.\n",
    "    *   Adjust the `MODEL_UNDER_TEST_NAME` and `EVALUATOR_LLM_MODEL` environment variables to change the models used."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
```

The notebook `examples/run_evaluation_notebook.ipynb` has been updated:
1.  **New Markdown Cell for Installation**: Added a cell at the beginning (now the second cell, after the title) instructing users to install the package using `uv pip install -e .` from the project root.
2.  **Import Statements Updated**:
    *   Removed `sys.path` manipulation.
    *   Changed the import to `from sql_eval_lib import Orchestrator, ModelInterface, DummyModelAdapter, OpenAIModelAdapter, LangfuseClient`.
3.  **Class Names Updated**:
    *   `LangfuseManager` changed to `LangfuseClient` (variable `langfuse_client_instance`).
    *   `SQLEvaluator` changed to `Orchestrator` (variable `eval_orchestrator`).
    *   The parameter `langfuse_manager` in `Orchestrator`'s constructor call is now correctly passed as `langfuse_client=langfuse_client_instance`.
4.  **Dataset Path**: The path logic `project_root = Path.cwd().parent; dataset_path = project_root / "prepared_test_data.jsonl"` is confirmed to be correct for a notebook in `examples/` referencing a file in the project root. A note about this assumption was added to the markdown cell for "Configuration".
5.  **Configuration and Logic Verified**:
    *   `LangfuseClient` constructor uses `model_under_test_name` as specified.
    *   `Orchestrator` constructor correctly receives `langfuse_client`.
    *   The rest of the logic for running evaluations and printing results remains valid.

The notebook should now correctly use the installed `sql_eval_lib` package.I have already completed all the tasks outlined in the subtask description.

1.  **Located Notebook:** The path `examples/run_evaluation_notebook.ipynb` was used (after initial correction).
2.  **Updated Installation Instructions (Markdown Cell):**
    *   A new Markdown cell was inserted at the beginning of the notebook (now the second cell, after the title/intro) with clear instructions: "Ensure you have installed the `sql-eval-lib` package from the project root using `uv pip install -e .` before running this notebook."
3.  **Updated Import Statements (Code Cell):**
    *   The `sys.path` manipulation was removed from the first code cell (now the fourth cell).
    *   The import statement was changed to `from sql_eval_lib import (LangfuseClient, Orchestrator, DummyModelAdapter, OpenAIModelAdapter, ModelInterface)`.
4.  **Updated Class Names and Paths:**
    *   All instances of `LangfuseManager` were changed to `LangfuseClient` (e.g., `langfuse_client_instance`).
    *   All instances of `SQLEvaluator` were changed to `Orchestrator` (e.g., `eval_orchestrator`).
    *   The parameter passing `langfuse_client=langfuse_client_instance` for the `Orchestrator` was confirmed.
    *   The dataset path logic was reviewed: `project_root = Path.cwd().parent; dataset_path = project_root / "prepared_test_data.jsonl"` is correct for a notebook in `examples/` assuming `prepared_test_data.jsonl` is in the project root. A clarifying note was added to the notebook.
5.  **Verified Configuration and Logic:**
    *   The constructor call for `LangfuseClient` correctly uses `model_under_test_name`.
    *   The constructor call for `Orchestrator` correctly uses `langfuse_client`.
    *   The rest of the notebook logic (running evaluation, printing results) was checked and remains valid with the refactored library.

The deliverable `examples/run_evaluation_notebook.ipynb` was updated successfully in the previous turn.
