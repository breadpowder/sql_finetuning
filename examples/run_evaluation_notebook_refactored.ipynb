{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç SECTION 1: Dataset Loading & Sample Exploration\n",
      "==================================================\n",
      "‚úÖ Dataset loaded successfully with 100000 items\n"
     ]
    }
   ],
   "source": [
    "# Section 1: Load Dataset and Show Samples\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('../sql_evaluation_library/src')\n",
    "\n",
    "from sql_eval_lib.dataset.models import DatasetItem\n",
    "from sql_eval_lib.dataset.loaders.huggingface_loader import HuggingFaceLoader\n",
    "import pandas as pd\n",
    "\n",
    "print(\"üîç SECTION 1: Dataset Loading & Sample Exploration\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "loader = HuggingFaceLoader()\n",
    "\n",
    "dataset = loader.load('gretelai/synthetic_text_to_sql', split='train')\n",
    "print(f\"‚úÖ Dataset loaded successfully with {len(dataset.items)} items\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 5097,\n",
       " 'domain': 'forestry',\n",
       " 'domain_description': 'Comprehensive data on sustainable forest management, timber production, wildlife habitat, and carbon sequestration in forestry.',\n",
       " 'sql_complexity': 'single join',\n",
       " 'sql_complexity_description': 'only one join (specify inner, outer, cross)',\n",
       " 'sql_task_type': 'analytics and reporting',\n",
       " 'sql_task_type_description': 'generating reports, dashboards, and analytical insights',\n",
       " 'sql_prompt': 'What is the total volume of timber sold by each salesperson, sorted by salesperson?',\n",
       " 'sql_context': \"CREATE TABLE salesperson (salesperson_id INT, name TEXT, region TEXT); INSERT INTO salesperson (salesperson_id, name, region) VALUES (1, 'John Doe', 'North'), (2, 'Jane Smith', 'South'); CREATE TABLE timber_sales (sales_id INT, salesperson_id INT, volume REAL, sale_date DATE); INSERT INTO timber_sales (sales_id, salesperson_id, volume, sale_date) VALUES (1, 1, 120, '2021-01-01'), (2, 1, 150, '2021-02-01'), (3, 2, 180, '2021-01-01');\",\n",
       " 'sql': 'SELECT salesperson_id, name, SUM(volume) as total_volume FROM timber_sales JOIN salesperson ON timber_sales.salesperson_id = salesperson.salesperson_id GROUP BY salesperson_id, name ORDER BY total_volume DESC;',\n",
       " 'sql_explanation': 'Joins timber_sales and salesperson tables, groups sales by salesperson, calculates total volume sold by each salesperson, and orders the results by total volume in descending order.'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.items[0].input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert to the format we need for evaluation\n",
    "sample_data = []\n",
    "\n",
    "for i, item in enumerate(dataset.items[:2]):  # Take first 2 samples\n",
    "    # The HuggingFaceLoader puts the raw data in the input field\n",
    "    raw_data = item.input\n",
    "    \n",
    "    # Convert to our evaluation format\n",
    "    converted_item = DatasetItem(\n",
    "        input={\n",
    "            \"question\": raw_data.get('sql_prompt', ''),\n",
    "            \"database_schema\": raw_data.get('sql_context', ''),  # Schema info is in sql_context\n",
    "            \"domain\": raw_data.get('domain', ''),\n",
    "            \"domain_description\": raw_data.get('domain_description', '')\n",
    "        },\n",
    "        expected_output={\n",
    "            \"sql\": raw_data.get('sql', '')\n",
    "        },\n",
    "        metadata={\n",
    "            \"difficulty\": raw_data.get('sql_complexity', 'unknown'),\n",
    "            \"category\": raw_data.get('sql_task_type', 'unknown'),\n",
    "            \"complexity_description\": raw_data.get('sql_complexity_description', ''),\n",
    "            \"task_description\": raw_data.get('sql_task_type_description', ''),\n",
    "            \"sql_explanation\": raw_data.get('sql_explanation', ''),\n",
    "            \"original_id\": raw_data.get('id', f'sample_{i+1}')\n",
    "        }\n",
    "    )\n",
    "    sample_data.append(converted_item)\n",
    "    \n",
    "\n",
    "\n",
    "print(f\"üìä Loaded {len(sample_data)} sample SQL evaluation items\")\n",
    "print(\"\\nüîç Sample Dataset Items:\")\n",
    "for i, item in enumerate(sample_data, 1):\n",
    "    print(f\"\\n--- Sample {i} ---\")\n",
    "    print(f\"Question: {item.input.get('question', 'N/A')}\")\n",
    "    print(f\"SQL: {item.expected_output.get('sql', 'N/A')}\")\n",
    "    print(f\"Difficulty: {item.metadata.get('difficulty', 'N/A')}\")\n",
    "    print(f\"Category: {item.metadata.get('category', 'N/A')}\")\n",
    "    \n",
    "    # Show domain info if available\n",
    "    domain = item.input.get('domain')\n",
    "    if domain:\n",
    "        print(f\"Domain: {domain}\")\n",
    "    \n",
    "    # Show schema (truncated if too long)\n",
    "    schema = item.input.get('database_schema', 'N/A')\n",
    "    if len(str(schema)) > 200:\n",
    "        print(f\"Schema: {str(schema)[:200]}...\")\n",
    "    else:\n",
    "        print(f\"Schema: {schema}\")\n",
    "    \n",
    "    # Show explanation if available\n",
    "    explanation = item.metadata.get('sql_explanation')\n",
    "    if explanation:\n",
    "        print(f\"Explanation: {explanation[:150]}...\")\n",
    "\n",
    "print(f\"\\n‚úÖ Section 1 Complete: Dataset loaded with {len(sample_data)} samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 2: Run Evaluation with 5 Samples (Without Langfuse)\n",
    "from sql_eval_lib.evaluation import SQLLLMEvaluator, SQLExecutionEvaluator, EvaluationSuite, SQLEvaluationContext\n",
    "from sql_eval_lib.evaluation.metrics import SQLSyntaxMetric, SQLExecutionAccuracyMetric, SemanticSimilarityMetric\n",
    "\n",
    "print(\"\\nüöÄ SECTION 2: Standalone SQL Evaluation\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Note: Add your OpenAI API key here\n",
    "OPENAI_API_KEY = \"your-openai-api-key-here\"  # Replace with your actual key\n",
    "\n",
    "# Initialize evaluation context\n",
    "eval_context = SQLEvaluationContext(\n",
    "    database_type=\"sqlite\",  # Using SQLite for demonstration\n",
    "    timeout_seconds=30\n",
    ")\n",
    "\n",
    "# Initialize evaluators\n",
    "print(\"üîß Initializing evaluators...\")\n",
    "\n",
    "try:\n",
    "    # LLM Evaluator for semantic and syntax analysis\n",
    "    llm_evaluator = SQLLLMEvaluator(\n",
    "        api_key=OPENAI_API_KEY,\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        max_tokens=1000\n",
    "    )\n",
    "    print(\"‚úÖ LLM Evaluator initialized\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è LLM Evaluator initialization failed: {e}\")\n",
    "    llm_evaluator = None\n",
    "\n",
    "# Execution Evaluator for SQL execution testing\n",
    "execution_evaluator = SQLExecutionEvaluator()\n",
    "print(\"‚úÖ Execution Evaluator initialized\")\n",
    "\n",
    "# Initialize metrics\n",
    "metrics = [\n",
    "    SQLSyntaxMetric(),\n",
    "    SQLExecutionAccuracyMetric(),\n",
    "    SemanticSimilarityMetric()\n",
    "]\n",
    "print(f\"‚úÖ {len(metrics)} metrics initialized\")\n",
    "\n",
    "# Create evaluation suite\n",
    "evaluation_suite = EvaluationSuite(\n",
    "    evaluators=[evaluator for evaluator in [llm_evaluator, execution_evaluator] if evaluator is not None],\n",
    "    metrics=metrics\n",
    ")\n",
    "\n",
    "# Run evaluation on first 5 samples\n",
    "print(f\"\\nüß™ Running evaluation on {min(5, len(sample_data))} samples...\")\n",
    "evaluation_results = []\n",
    "\n",
    "for i, item in enumerate(sample_data[:5], 1):\n",
    "    print(f\"\\n--- Evaluating Sample {i} ---\")\n",
    "    print(f\"Question: {item.input.get('question', 'N/A')}\")\n",
    "    \n",
    "    try:\n",
    "        # Run evaluation\n",
    "        result = evaluation_suite.evaluate_item(item, eval_context)\n",
    "        evaluation_results.append({\n",
    "            'sample_id': i,\n",
    "            'question': item.input.get('question', 'N/A'),\n",
    "            'result': result,\n",
    "            'status': 'success'\n",
    "        })\n",
    "        print(f\"‚úÖ Sample {i} evaluated successfully\")\n",
    "        \n",
    "        # Display basic results\n",
    "        if hasattr(result, 'metrics'):\n",
    "            for metric_name, metric_value in result.metrics.items():\n",
    "                print(f\"  {metric_name}: {metric_value}\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Sample {i} evaluation failed: {e}\")\n",
    "        evaluation_results.append({\n",
    "            'sample_id': i,\n",
    "            'question': item.input.get('question', 'N/A'),\n",
    "            'error': str(e),\n",
    "            'status': 'failed'\n",
    "        })\n",
    "\n",
    "# Summary\n",
    "successful_evaluations = len([r for r in evaluation_results if r['status'] == 'success'])\n",
    "print(f\"\\nüìä Evaluation Summary:\")\n",
    "print(f\"  Total samples: {len(evaluation_results)}\")\n",
    "print(f\"  Successful: {successful_evaluations}\")\n",
    "print(f\"  Failed: {len(evaluation_results) - successful_evaluations}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Section 2 Complete: Standalone evaluation finished\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 3: Ensure Langfuse Self-Hosted is Ready\n",
    "import subprocess\n",
    "import time\n",
    "import requests\n",
    "from sql_eval_lib.langfuse import LangfuseDeployment\n",
    "\n",
    "print(\"\\nüê≥ SECTION 3: Langfuse Self-Hosted Setup Verification\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Check if Docker is available\n",
    "def check_docker():\n",
    "    try:\n",
    "        result = subprocess.run(['docker', '--version'], capture_output=True, text=True)\n",
    "        if result.returncode == 0:\n",
    "            print(f\"‚úÖ Docker is available: {result.stdout.strip()}\")\n",
    "            return True\n",
    "        else:\n",
    "            print(\"‚ùå Docker is not available\")\n",
    "            return False\n",
    "    except FileNotFoundError:\n",
    "        print(\"‚ùå Docker command not found\")\n",
    "        return False\n",
    "\n",
    "# Check if Docker Compose is available\n",
    "def check_docker_compose():\n",
    "    try:\n",
    "        result = subprocess.run(['docker-compose', '--version'], capture_output=True, text=True)\n",
    "        if result.returncode == 0:\n",
    "            print(f\"‚úÖ Docker Compose is available: {result.stdout.strip()}\")\n",
    "            return True\n",
    "        else:\n",
    "            # Try docker compose (newer syntax)\n",
    "            result = subprocess.run(['docker', 'compose', 'version'], capture_output=True, text=True)\n",
    "            if result.returncode == 0:\n",
    "                print(f\"‚úÖ Docker Compose is available: {result.stdout.strip()}\")\n",
    "                return True\n",
    "            print(\"‚ùå Docker Compose is not available\")\n",
    "            return False\n",
    "    except FileNotFoundError:\n",
    "        print(\"‚ùå Docker Compose command not found\")\n",
    "        return False\n",
    "\n",
    "# Check for existing docker-compose.yml\n",
    "def check_docker_compose_file():\n",
    "    docker_compose_path = \"../docker-compose.yml\"\n",
    "    if os.path.exists(docker_compose_path):\n",
    "        print(f\"‚úÖ Found docker-compose.yml at {docker_compose_path}\")\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"‚ùå No docker-compose.yml found at {docker_compose_path}\")\n",
    "        return False\n",
    "\n",
    "print(\"üîç Checking Docker environment...\")\n",
    "docker_available = check_docker()\n",
    "docker_compose_available = check_docker_compose()\n",
    "docker_compose_file_exists = check_docker_compose_file()\n",
    "\n",
    "if all([docker_available, docker_compose_available, docker_compose_file_exists]):\n",
    "    print(\"\\nüöÄ Attempting to deploy Langfuse...\")\n",
    "    \n",
    "    try:\n",
    "        # Initialize Langfuse deployment\n",
    "        deployment = LangfuseDeployment(\n",
    "            compose_file_path=\"../docker-compose.yml\",\n",
    "            project_name=\"sql_eval_langfuse\"\n",
    "        )\n",
    "        \n",
    "        print(\"üì¶ Starting Langfuse deployment...\")\n",
    "        deployment.deploy()\n",
    "        \n",
    "        print(\"‚è≥ Waiting for Langfuse to become healthy...\")\n",
    "        if deployment.wait_for_healthy(timeout=120):\n",
    "            print(\"‚úÖ Langfuse is running and healthy!\")\n",
    "            \n",
    "            # Test connection\n",
    "            try:\n",
    "                response = requests.get(\"http://localhost:3000/api/public/health\", timeout=10)\n",
    "                if response.status_code == 200:\n",
    "                    print(\"‚úÖ Langfuse API is responding\")\n",
    "                else:\n",
    "                    print(f\"‚ö†Ô∏è Langfuse API returned status {response.status_code}\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Could not test Langfuse API: {e}\")\n",
    "                \n",
    "        else:\n",
    "            print(\"‚ùå Langfuse failed to become healthy within timeout\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Langfuse deployment failed: {e}\")\n",
    "        print(\"üí° You may need to:\")\n",
    "        print(\"   - Check Docker daemon is running\")\n",
    "        print(\"   - Ensure ports 3000 and 5432 are available\")\n",
    "        print(\"   - Review docker-compose.yml configuration\")\n",
    "        \n",
    "else:\n",
    "    print(\"\\n‚ùå Prerequisites not met for Langfuse deployment\")\n",
    "    print(\"üí° Required:\")\n",
    "    print(\"   - Docker installed and running\")\n",
    "    print(\"   - Docker Compose available\")\n",
    "    print(\"   - docker-compose.yml file present\")\n",
    "\n",
    "print(f\"\\n‚úÖ Section 3 Complete: Langfuse setup verification finished\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 4: Create Dataset with 5 Samples Through Langfuse\n",
    "from sql_eval_lib.langfuse import LangfuseClient\n",
    "from sql_eval_lib.dataset.langfuse_integration import LangfuseDatasetManager\n",
    "\n",
    "print(\"\\nüîó SECTION 4: Langfuse-Integrated Evaluation\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Langfuse configuration\n",
    "LANGFUSE_PUBLIC_KEY = \"pk-lf-1234567890abcdef\"  # Replace with actual key\n",
    "LANGFUSE_SECRET_KEY = \"sk-lf-1234567890abcdef\"  # Replace with actual key\n",
    "LANGFUSE_HOST = \"http://localhost:3000\"\n",
    "\n",
    "try:\n",
    "    print(\"üîß Initializing Langfuse client...\")\n",
    "    \n",
    "    # Initialize Langfuse client\n",
    "    langfuse_client = LangfuseClient(\n",
    "        public_key=LANGFUSE_PUBLIC_KEY,\n",
    "        secret_key=LANGFUSE_SECRET_KEY,\n",
    "        host=LANGFUSE_HOST\n",
    "    )\n",
    "    \n",
    "    # Test connection\n",
    "    print(\"üîç Testing Langfuse connection...\")\n",
    "    if langfuse_client.test_connection():\n",
    "        print(\"‚úÖ Langfuse connection successful!\")\n",
    "        \n",
    "        # Initialize dataset manager\n",
    "        dataset_manager = LangfuseDatasetManager(langfuse_client)\n",
    "        \n",
    "        # Create a new dataset\n",
    "        dataset_name = f\"sql_evaluation_samples_{int(time.time())}\"\n",
    "        print(f\"\\nüìä Creating dataset: {dataset_name}\")\n",
    "        \n",
    "        try:\n",
    "            # Create dataset in Langfuse\n",
    "            dataset_id = dataset_manager.create_dataset(\n",
    "                name=dataset_name,\n",
    "                description=\"SQL evaluation samples for interactive notebook testing\"\n",
    "            )\n",
    "            print(f\"‚úÖ Dataset created with ID: {dataset_id}\")\n",
    "            \n",
    "            # Upload sample data to Langfuse\n",
    "            print(f\"\\nüì§ Uploading {len(sample_data)} samples to Langfuse...\")\n",
    "            \n",
    "            uploaded_count = 0\n",
    "            for i, item in enumerate(sample_data, 1):\n",
    "                try:\n",
    "                    # DatasetItem is already in the correct format for Langfuse\n",
    "                    langfuse_item = {\n",
    "                        \"input\": item.input,\n",
    "                        \"expected_output\": item.expected_output,\n",
    "                        \"metadata\": item.metadata\n",
    "                    }\n",
    "                    \n",
    "                    # Upload to Langfuse\n",
    "                    item_id = dataset_manager.add_item(\n",
    "                        dataset_id=dataset_id,\n",
    "                        item=langfuse_item\n",
    "                    )\n",
    "                    \n",
    "                    print(f\"‚úÖ Sample {i} uploaded (ID: {item_id})\")\n",
    "                    uploaded_count += 1\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"‚ùå Failed to upload sample {i}: {e}\")\n",
    "            \n",
    "            print(f\"\\nüìä Upload Summary:\")\n",
    "            print(f\"  Total samples: {len(sample_data)}\")\n",
    "            print(f\"  Successfully uploaded: {uploaded_count}\")\n",
    "            print(f\"  Failed: {len(sample_data) - uploaded_count}\")\n",
    "            \n",
    "            # Retrieve and verify dataset\n",
    "            print(f\"\\nüîç Verifying dataset in Langfuse...\")\n",
    "            try:\n",
    "                retrieved_items = dataset_manager.get_dataset_items(dataset_id)\n",
    "                print(f\"‚úÖ Verified: {len(retrieved_items)} items in dataset\")\n",
    "                \n",
    "                # Show first item as verification\n",
    "                if retrieved_items:\n",
    "                    first_item = retrieved_items[0]\n",
    "                    print(f\"\\nüîç First item verification:\")\n",
    "                    print(f\"  Question: {first_item.get('input', {}).get('question', 'N/A')}\")\n",
    "                    print(f\"  Expected SQL: {first_item.get('expected_output', 'N/A')[:100]}...\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Could not verify dataset: {e}\")\n",
    "            \n",
    "            # Run evaluation with Langfuse tracking\n",
    "            print(f\"\\nüß™ Running Langfuse-tracked evaluation...\")\n",
    "            \n",
    "            langfuse_evaluation_results = []\n",
    "            for i, item in enumerate(sample_data[:3], 1):  # Test with 3 samples\n",
    "                print(f\"\\n--- Langfuse Evaluation Sample {i} ---\")\n",
    "                \n",
    "                try:\n",
    "                    # Create a trace in Langfuse for this evaluation\n",
    "                    trace = langfuse_client.trace(\n",
    "                        name=f\"sql_evaluation_sample_{i}\",\n",
    "                        metadata={\"sample_id\": i, \"dataset\": dataset_name}\n",
    "                    )\n",
    "                    \n",
    "                    # Run evaluation (reuse previous evaluation logic)\n",
    "                    result = evaluation_suite.evaluate_item(item, eval_context)\n",
    "                    \n",
    "                    # Log results to Langfuse\n",
    "                    trace.generation(\n",
    "                        name=\"sql_evaluation\",\n",
    "                        input={\"question\": item.input.get('question'), \"sql\": item.expected_output.get('sql')},\n",
    "                        output={\"evaluation_result\": str(result)},\n",
    "                        metadata=item.metadata\n",
    "                    )\n",
    "                    \n",
    "                    langfuse_evaluation_results.append({\n",
    "                        'sample_id': i,\n",
    "                        'trace_id': trace.id,\n",
    "                        'status': 'success'\n",
    "                    })\n",
    "                    \n",
    "                    print(f\"‚úÖ Sample {i} evaluated and logged to Langfuse\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"‚ùå Sample {i} Langfuse evaluation failed: {e}\")\n",
    "                    langfuse_evaluation_results.append({\n",
    "                        'sample_id': i,\n",
    "                        'error': str(e),\n",
    "                        'status': 'failed'\n",
    "                    })\n",
    "            \n",
    "            # Final summary\n",
    "            successful_langfuse = len([r for r in langfuse_evaluation_results if r['status'] == 'success'])\n",
    "            print(f\"\\nüìä Langfuse Evaluation Summary:\")\n",
    "            print(f\"  Dataset created: {dataset_name}\")\n",
    "            print(f\"  Items uploaded: {uploaded_count}/{len(sample_data)}\")\n",
    "            print(f\"  Evaluations tracked: {successful_langfuse}/{len(langfuse_evaluation_results)}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Dataset creation/upload failed: {e}\")\n",
    "            \n",
    "    else:\n",
    "        print(\"‚ùå Langfuse connection failed\")\n",
    "        print(\"üí° Please check:\")\n",
    "        print(\"   - Langfuse is running (Section 3)\")\n",
    "        print(\"   - API keys are correct\")\n",
    "        print(\"   - Host URL is accessible\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Langfuse client initialization failed: {e}\")\n",
    "    print(\"üí° Please ensure:\")\n",
    "    print(\"   - Langfuse is deployed and running\")\n",
    "    print(\"   - API credentials are configured\")\n",
    "    print(\"   - Network connectivity to Langfuse host\")\n",
    "\n",
    "print(f\"\\n‚úÖ Section 4 Complete: Langfuse-integrated evaluation finished\")\n",
    "\n",
    "print(f\"\\nüéâ ALL SECTIONS COMPLETE!\")\n",
    "print(\"=\" * 50)\n",
    "print(\"üìã Summary of completed sections:\")\n",
    "print(\"  ‚úÖ Section 1: Dataset loading and sample exploration\")\n",
    "print(\"  ‚úÖ Section 2: Standalone SQL evaluation\")\n",
    "print(\"  ‚úÖ Section 3: Langfuse deployment verification\")\n",
    "print(\"  ‚úÖ Section 4: Langfuse-integrated evaluation\")\n",
    "print(\"\\nüöÄ The interactive SQL evaluation notebook is ready for use!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sql_fine_tuning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
